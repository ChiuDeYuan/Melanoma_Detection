{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChiuDeYuan/Melanoma_Detection/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nc5a8tZ8k55Y"
      },
      "source": [
        "## Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dhOv2Dc-NKOf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
        "\n",
        "import keras\n",
        "from keras import layers\n",
        "from keras import backend\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import itertools\n",
        "import scipy\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uDF0brtSjXZi"
      },
      "outputs": [],
      "source": [
        "print(keras.__version__)    #should be 2.10.0\n",
        "print(tf.__version__)       #should be 2.10.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cjxG_Z-VZeRx"
      },
      "outputs": [],
      "source": [
        "#policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
        "#tf.keras.mixed_precision.set_global_policy(policy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VhQkPfSkqLB"
      },
      "source": [
        "## Mobile ViT\n",
        "\n",
        "### The Mobile ViT model is modified from Keras example.\n",
        "link: https://keras.io/examples/vision/mobilevit/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5hXPBVYFfPCr"
      },
      "outputs": [],
      "source": [
        "# Values are from table 4.\n",
        "patch_size = 4  # 2x2, for the Transformer blocks.\n",
        "image_size = 256\n",
        "expansion_factor = 2  # expansion factor for the MobileNetV2 blocks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QZnOBrtqfWOX"
      },
      "outputs": [],
      "source": [
        "def conv_block(x, filters=16, kernel_size=3, strides=2):\n",
        "    conv_layer = layers.Conv2D(\n",
        "        filters,\n",
        "        kernel_size,\n",
        "        strides=strides,\n",
        "        activation=keras.activations.swish,\n",
        "        padding=\"same\",\n",
        "    )\n",
        "    return conv_layer(x)\n",
        "\n",
        "\n",
        "# Reference: https://github.com/keras-team/keras/blob/e3858739d178fe16a0c77ce7fab88b0be6dbbdc7/keras/applications/imagenet_utils.py#L413C17-L435\n",
        "\n",
        "\n",
        "def correct_pad(inputs, kernel_size):\n",
        "    img_dim = 2 if backend.image_data_format() == \"channels_first\" else 1\n",
        "    input_size = inputs.shape[img_dim : (img_dim + 2)]\n",
        "    if isinstance(kernel_size, int):\n",
        "        kernel_size = (kernel_size, kernel_size)\n",
        "    if input_size[0] is None:\n",
        "        adjust = (1, 1)\n",
        "    else:\n",
        "        adjust = (1 - input_size[0] % 2, 1 - input_size[1] % 2)\n",
        "    correct = (kernel_size[0] // 2, kernel_size[1] // 2)\n",
        "    return (\n",
        "        (correct[0] - adjust[0], correct[0]),\n",
        "        (correct[1] - adjust[1], correct[1]),\n",
        "    )\n",
        "\n",
        "\n",
        "# Reference: https://git.io/JKgtC\n",
        "\n",
        "\n",
        "def inverted_residual_block(x, expanded_channels, output_channels, strides=1):\n",
        "    m = layers.Conv2D(expanded_channels, 1, padding=\"same\", use_bias=False)(x)\n",
        "    m = layers.BatchNormalization()(m)\n",
        "    m = keras.activations.swish(m)\n",
        "\n",
        "    if strides == 2:\n",
        "        m = layers.ZeroPadding2D(padding=correct_pad(m, 3))(m)\n",
        "    m = layers.DepthwiseConv2D(\n",
        "        3, strides=strides, padding=\"same\" if strides == 1 else \"valid\", use_bias=False\n",
        "    )(m)\n",
        "    m = layers.BatchNormalization()(m)\n",
        "    m = keras.activations.swish(m)\n",
        "\n",
        "    m = layers.Conv2D(output_channels, 1, padding=\"same\", use_bias=False)(m)\n",
        "    m = layers.BatchNormalization()(m)\n",
        "\n",
        "    if tf.equal(x.shape[-1], output_channels) and strides == 1:\n",
        "        return layers.Add()([m, x])\n",
        "    return m\n",
        "\n",
        "\n",
        "# Reference:\n",
        "# https://keras.io/examples/vision/image_classification_with_vision_transformer/\n",
        "\n",
        "\n",
        "def mlp(x, hidden_units, dropout_rate):\n",
        "    for units in hidden_units:\n",
        "        x = layers.Dense(units, activation=keras.activations.gelu)(x)   #activation must be gelu\n",
        "        x = layers.Dropout(dropout_rate)(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def transformer_block(x, transformer_layers, projection_dim, num_heads=4):\n",
        "    for _ in range(transformer_layers):\n",
        "        # Layer normalization 1.\n",
        "        x1 = layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "        # Create a multi-head attention layer.\n",
        "        attention_output = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
        "        )(x1, x1)\n",
        "        # Skip connection 1.\n",
        "        x2 = layers.Add()([attention_output, x])\n",
        "        # Layer normalization 2.\n",
        "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
        "        # MLP.\n",
        "        x3 = mlp(\n",
        "            x3,\n",
        "            hidden_units=[x.shape[-1] * 2, x.shape[-1]],\n",
        "            dropout_rate=0.1,\n",
        "        )\n",
        "        # Skip connection 2.\n",
        "        x = layers.Add()([x3, x2])\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def mobilevit_block(x, num_blocks, projection_dim, strides=1):\n",
        "    # Local projection with convolutions.\n",
        "    local_features = conv_block(x, filters=projection_dim, strides=strides)\n",
        "    local_features = conv_block(\n",
        "        local_features, filters=projection_dim, kernel_size=1, strides=strides\n",
        "    )\n",
        "\n",
        "    # Unfold into patches and then pass through Transformers.\n",
        "    num_patches = int((local_features.shape[1] * local_features.shape[2]) / patch_size)\n",
        "    non_overlapping_patches = layers.Reshape((patch_size, num_patches, projection_dim))(\n",
        "        local_features\n",
        "    )\n",
        "    global_features = transformer_block(\n",
        "        non_overlapping_patches, num_blocks, projection_dim\n",
        "    )\n",
        "\n",
        "    # Fold into conv-like feature-maps.\n",
        "    folded_feature_map = layers.Reshape((*local_features.shape[1:-1], projection_dim))(\n",
        "        global_features\n",
        "    )\n",
        "\n",
        "    # Apply point-wise conv -> concatenate with the input features.\n",
        "    folded_feature_map = conv_block(\n",
        "        folded_feature_map, filters=x.shape[-1], kernel_size=1, strides=strides\n",
        "    )\n",
        "    local_global_features = layers.Concatenate(axis=-1)([x, folded_feature_map])\n",
        "\n",
        "    # Fuse the local and global features using a convoluion layer.\n",
        "    local_global_features = conv_block(\n",
        "        local_global_features, filters=projection_dim, strides=strides\n",
        "    )\n",
        "\n",
        "    return local_global_features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wixZQi4DfYkt"
      },
      "outputs": [],
      "source": [
        "def create_mobilevit(num_classes=2):\n",
        "    inputs = keras.Input((image_size, image_size, 3))\n",
        "    x = layers.Rescaling(scale=1.0 / 255)(inputs)\n",
        "\n",
        "    # Initial conv-stem -> MV2 block.\n",
        "    x = conv_block(x, filters=16)\n",
        "    x = inverted_residual_block(\n",
        "        x, expanded_channels=16 * expansion_factor, output_channels=16\n",
        "    )\n",
        "\n",
        "    # Downsampling with MV2 block.\n",
        "    x = inverted_residual_block(\n",
        "        x, expanded_channels=16 * expansion_factor, output_channels=24, strides=2\n",
        "    )\n",
        "    x = inverted_residual_block(\n",
        "        x, expanded_channels=24 * expansion_factor, output_channels=24\n",
        "    )\n",
        "    x = inverted_residual_block(\n",
        "        x, expanded_channels=24 * expansion_factor, output_channels=24\n",
        "    )\n",
        "\n",
        "    # First MV2 -> MobileViT block.\n",
        "    x = inverted_residual_block(\n",
        "        x, expanded_channels=24 * expansion_factor, output_channels=48, strides=2\n",
        "    )\n",
        "    x = mobilevit_block(x, num_blocks=2, projection_dim=64)\n",
        "\n",
        "    # Second MV2 -> MobileViT block.\n",
        "    x = inverted_residual_block(\n",
        "        x, expanded_channels=64 * expansion_factor, output_channels=64, strides=2\n",
        "    )\n",
        "    x = mobilevit_block(x, num_blocks=4, projection_dim=80)\n",
        "\n",
        "    # Third MV2 -> MobileViT block.\n",
        "    x = inverted_residual_block(\n",
        "        x, expanded_channels=80 * expansion_factor, output_channels=80, strides=2\n",
        "    )\n",
        "    x = mobilevit_block(x, num_blocks=3, projection_dim=96)\n",
        "    x = conv_block(x, filters=320, kernel_size=1, strides=1)\n",
        "\n",
        "    # Classification head.\n",
        "    x = layers.GlobalAvgPool2D()(x)\n",
        "    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
        "\n",
        "    return keras.Model(inputs, outputs)\n",
        "\n",
        "\n",
        "mobilevit_xxs = create_mobilevit()\n",
        "mobilevit_xxs.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23cu1XX8p5p8"
      },
      "source": [
        "## Prepare"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A_dHEpaqZeRz"
      },
      "outputs": [],
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "print(device_lib.list_local_devices())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ra935piIZeRz"
      },
      "outputs": [],
      "source": [
        "config = tf.compat.v1.ConfigProto()\n",
        "config.gpu_options.per_process_gpu_memory_fraction = 0.9  # 0.6 sometimes works better for folks\n",
        "keras.backend.set_session(tf.compat.v1.Session(config=config))\n",
        "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2m2H5a_iZeRz"
      },
      "outputs": [],
      "source": [
        "base_dir = './melanoma_cancer_dataset/train'\n",
        "benign_dir = os.path.join(base_dir, 'benign')\n",
        "malignant_dir = os.path.join(base_dir, 'malignant')\n",
        "\n",
        "file_paths = []\n",
        "labels = []\n",
        "\n",
        "# adding \"benign\" image paths and labels\n",
        "for filename in os.listdir(benign_dir):\n",
        "    if filename.endswith('.jpg') or filename.endswith('.png'):\n",
        "        file_paths.append(os.path.join(benign_dir, filename))\n",
        "        labels.append('benign')\n",
        "\n",
        "# adding \"malignant\" image paths and labels\n",
        "for filename in os.listdir(malignant_dir):\n",
        "    if filename.endswith('.jpg') or filename.endswith('.png'):\n",
        "        file_paths.append(os.path.join(malignant_dir, filename))\n",
        "        labels.append('malignant')\n",
        "\n",
        "# create DataFrame\n",
        "train_df = pd.DataFrame({\n",
        "    'file_path': file_paths,\n",
        "    'label': labels\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z3sbKSaHZeRz"
      },
      "outputs": [],
      "source": [
        "train_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "swdgftcNZeRz"
      },
      "outputs": [],
      "source": [
        "base_test_dir = './melanoma_cancer_dataset/test'\n",
        "benign_test_dir = os.path.join(base_test_dir, 'benign')\n",
        "malignant_test_dir = os.path.join(base_test_dir, 'malignant')\n",
        "\n",
        "test_file_paths = []\n",
        "test_labels = []\n",
        "\n",
        "for filename in os.listdir(benign_test_dir):\n",
        "    if filename.endswith('.jpg') or filename.endswith('.png'):\n",
        "        test_file_paths.append(os.path.join(benign_test_dir, filename))\n",
        "        test_labels.append('benign')\n",
        "\n",
        "for filename in os.listdir(malignant_test_dir):\n",
        "    if filename.endswith('.jpg') or filename.endswith('.png'):\n",
        "        test_file_paths.append(os.path.join(malignant_test_dir, filename))\n",
        "        test_labels.append('malignant')\n",
        "\n",
        "test_df = pd.DataFrame({\n",
        "    'file_path': test_file_paths,\n",
        "    'label': test_labels\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5UparHXGZeR0"
      },
      "outputs": [],
      "source": [
        "test_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gq9OFttMZeR0"
      },
      "outputs": [],
      "source": [
        "# define function to display images from a DataFrame\n",
        "def display_sample_images(df, label, num_images=5):\n",
        "    # Filter DataFrame by the label\n",
        "    label_df = df[df['label'] == label]\n",
        "\n",
        "\n",
        "    sample_df = label_df.sample(num_images, random_state=1)\n",
        "\n",
        "\n",
        "    plt.figure(figsize=(15, 5))\n",
        "\n",
        "    for i, (index, row) in enumerate(sample_df.iterrows()):\n",
        "        img = mpimg.imread(row['file_path'])\n",
        "        plt.subplot(1, num_images, i + 1)\n",
        "        plt.imshow(img)\n",
        "        plt.axis('off')\n",
        "        plt.title(label)\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LGUwhlK0ZeR0"
      },
      "outputs": [],
      "source": [
        "display_sample_images(train_df, 'benign')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hsbtgjXDZeR0"
      },
      "outputs": [],
      "source": [
        "display_sample_images(train_df, 'malignant')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yxc69ptPZeR0"
      },
      "outputs": [],
      "source": [
        "train_df, val_df = train_test_split(train_df, test_size=0.1, stratify=train_df['label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1kX1AK6UZeR0"
      },
      "outputs": [],
      "source": [
        "train_datagen = ImageDataGenerator(\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "val_datagen = ImageDataGenerator()\n",
        "test_datagen = ImageDataGenerator()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nq5mKvRCZeR0"
      },
      "outputs": [],
      "source": [
        "train_generator = train_datagen.flow_from_dataframe(\n",
        "    dataframe=train_df,\n",
        "    x_col='file_path',\n",
        "    y_col='label',\n",
        "    target_size=(256, 256),\n",
        "    batch_size=4,\n",
        "    class_mode='categorical',\n",
        "    color_mode='rgb',\n",
        ")\n",
        "\n",
        "val_generator = val_datagen.flow_from_dataframe(\n",
        "    dataframe=val_df,\n",
        "    x_col='file_path',\n",
        "    y_col='label',\n",
        "    target_size=(256, 256),\n",
        "    batch_size=4,\n",
        "    class_mode='categorical',\n",
        "    color_mode='rgb'\n",
        ")\n",
        "\n",
        "test_generator = test_datagen.flow_from_dataframe(\n",
        "    dataframe=test_df,\n",
        "    x_col='file_path',\n",
        "    y_col='label',\n",
        "    target_size=(256, 256),\n",
        "    batch_size=4,\n",
        "    class_mode='categorical',\n",
        "    color_mode='rgb',\n",
        "    shuffle=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWM6GJpMZeR0"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FiBxtGaagjjV"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.00025\n",
        "label_smoothing_factor = 0.1\n",
        "epochs = 10\n",
        "num_classes = 2\n",
        "\n",
        "optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "loss_fn = keras.losses.CategoricalCrossentropy(label_smoothing=label_smoothing_factor)\n",
        "\n",
        "\n",
        "def run_experiment(epochs=epochs):\n",
        "    mobilevit_xxs = create_mobilevit(num_classes=num_classes)\n",
        "    mobilevit_xxs.compile(optimizer=optimizer, loss=loss_fn, metrics=[\"accuracy\", keras.metrics.AUC(from_logits=True)])\n",
        "\n",
        "    # When using `save_weights_only=True` in `ModelCheckpoint`, the filepath provided must end in `.weights.h5`\n",
        "    checkpoint_filepath = \"./tmp/checkpoint.weights.h5\"\n",
        "    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
        "        checkpoint_filepath,\n",
        "        monitor=\"val_accuracy\",\n",
        "        save_best_only=True,\n",
        "        save_weights_only=True,\n",
        "    )\n",
        "\n",
        "    history = mobilevit_xxs.fit(\n",
        "        train_generator,\n",
        "        validation_data=val_generator,\n",
        "        epochs=epochs,\n",
        "        callbacks=[checkpoint_callback],\n",
        "    )\n",
        "    mobilevit_xxs.load_weights(checkpoint_filepath)\n",
        "    _, accuracy, auc = mobilevit_xxs.evaluate(test_generator)\n",
        "    print(f\"Validation accuracy: {round(accuracy * 100, 2)}%\")\n",
        "    print(f\"Validation auc: {round(auc * 100, 2)}%\")\n",
        "    return mobilevit_xxs, history\n",
        "\n",
        "\n",
        "mobilevit_xxs, history = run_experiment()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J6nsROjYZeR1"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(6, 12))\n",
        "plt.subplot(311)\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.subplot(312)\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "plt.subplot(313)\n",
        "plt.plot(history.history['auc'])\n",
        "plt.plot(history.history['val_auc'])\n",
        "plt.title('Model AUC')\n",
        "plt.ylabel('AUC')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NmlxLuW6ZeR1"
      },
      "outputs": [],
      "source": [
        "mobilevit_xxs.evaluate(test_generator)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KRDeB-yu8M2"
      },
      "source": [
        "## Converter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B2TrgKpvZeR1"
      },
      "outputs": [],
      "source": [
        "mobilevit_xxs = tf.keras.models.load_model(\"./melanoma_mobilevit_xxs_float32.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w9yC278_ZeR1"
      },
      "outputs": [],
      "source": [
        "mobilevit_xxs.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vR3Hc4s4u_Ho"
      },
      "outputs": [],
      "source": [
        "# Serialize the model as a SavedModel.\n",
        "tf.saved_model.save(mobilevit_xxs, \"melanoma_mobilevit_xxs\")\n",
        "\n",
        "# Convert to TFLite. This form of quantization is called\n",
        "# post-training dynamic-range quantization in TFLite.\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(\"melanoma_mobilevit_xxs\")\n",
        "converter.allow_custom_ops=True\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.target_spec.supported_ops = [\n",
        "    tf.lite.OpsSet.TFLITE_BUILTINS,  # Enable TensorFlow Lite ops.\n",
        "    tf.lite.OpsSet.SELECT_TF_OPS,  # Enable TensorFlow ops.\n",
        "]\n",
        "tflite_model = converter.convert()\n",
        "open(\"melanoma_mobilevit_xxs.tflite\", \"wb\").write(tflite_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EIJmscV8ZeR2"
      },
      "outputs": [],
      "source": [
        "mobilevit_xxs.save"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEX6_mPGZeR2"
      },
      "source": [
        "## Clear"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nT9eZpwiZeR2"
      },
      "outputs": [],
      "source": [
        "backend.clear_session()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "nc5a8tZ8k55Y",
        "7VhQkPfSkqLB"
      ],
      "provenance": [],
      "name": "main.ipynb",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}